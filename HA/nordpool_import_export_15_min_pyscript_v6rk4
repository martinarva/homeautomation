"""
NordPool 15-min (EE) via Pyscript — single raw_all stream with smart polling & date validation

Entities (create these 3 helper sensors in HA first):
- sensor.nordpool_15_min_raw_pyscript
  state  : current 15-min market price (€/kWh), derived from cached data if possible
  attrs  : raw_all (local today + local tomorrow + spill hour)
           updatedAt_tm_utc, updatedAt_tm_local (publish time of "tomorrow")
           status_y, status_t, status_tm (Final / Preliminary / Missing / Corrected / Cancelled / Unknown(...))
           areaStates_y, areaStates_t, areaStates_tm (raw areaStates from API)
           slots_today, slots_tomorrow, slots_all, expected_slots_today, expected_slots_tomorrow
           today_first_slot, today_last_slot, today_complete
           tm_first_slot, tm_last_slot, tomorrow_complete, tm_stale
           today_date_local, tomorrow_date_local_expected
           last_fetch_at_local

- sensor.nordpool_15_min_import_pyscript
  state  : current 15-min import price after tariffs (€/kWh)
  attrs  : raw_all (tariffed view), updatedAt_tm_* and status fields mirrored

- sensor.nordpool_15_min_export_pyscript
  state  : current 15-min export price (market + export margin; no VAT), €/kWh
  attrs  : raw_all (export-adjusted view), updatedAt_tm_* and status fields mirrored

Scheduling:
- Runs on startup
- Every 15 minutes (we decide inside whether to call the API)
- We only chase "tomorrow" between 13:00–00:00 local, or when status_tm is not Final.
  (Prices are typically published in this window; we poll more only to catch the update promptly.)

Notes:
- raw_all includes the extra **spill hour** that local time gets across CET↔EE.
- We avoid unnecessary API calls by checking completeness & status first.
- From midnight on, we validate by DATE WINDOWS (start/end/contiguity), not just counts.
"""

import json
import datetime as dt
import aiohttp
import asyncio
import random

# ---------- Configuration ----------
HOLIDAY_CALENDAR = "calendar.estonia"  # set to None to disable holiday logic (weekend/night remains)
AREA       = "EE"
CURRENCY   = "EUR"
RESOLUTION = 15  # minutes

# Import tariff components (EUR/kWh)
IMPORT = {
    "alexela_marginaal": 0.0000,
    "taastuv":          0.0084,
    "aktsiis":          0.0021,
    "elektrilevi_p2ev": 0.0369,  # day tariff
    "elektrilevi_88":   0.0210,  # night/weekend/holiday
    "vat":              1.24,    # multiplier
}

# Export margin (negative → subtract from market). No VAT for export.
EXPORT_MARGIN = -0.00167  # EUR/kWh

# Target helper entities (must exist in HA)
SENSOR_RAW    = "sensor.nordpool_15_min_raw_pyscript"
SENSOR_IMPORT = "sensor.nordpool_15_min_import_pyscript"
SENSOR_EXPORT = "sensor.nordpool_15_min_export_pyscript"

# ---------- Timezones ----------
try:
    from zoneinfo import ZoneInfo
    EE_TZ  = ZoneInfo("Europe/Tallinn")
    CET_TZ = ZoneInfo("Europe/Paris")   # CET/CEST (market calendar)
except Exception:
    EE_TZ  = dt.timezone(dt.timedelta(hours=3))
    CET_TZ = dt.timezone(dt.timedelta(hours=1))

API_BASE = "https://dataportal-api.nordpoolgroup.com/api/DayAheadPriceIndices"
HEADERS  = {
    "Accept": "application/json",
    "User-Agent": "HomeAssistant-Pyscript-Nordpool/1.4 (+nordpool_15_min_raw)"
}

# ---------- HTTP / parsing helpers ----------
async def _fetch_date(session, date, area, currency, resolution) -> dict | None:
    url = (
        f"{API_BASE}?date={date}"
        f"&market=DayAhead&indexNames={area}&currency={currency}&resolutionInMinutes={resolution}"
    )
    log.info(f"🔗 GET {url}")
    try:
        async with session.get(url, headers=HEADERS, timeout=30) as resp:
            body = await resp.text()
            if resp.status != 200:
                log.warning(f"HTTP {resp.status} for {date}; body[:200]={body[:200]}")
                return None
            try:
                return json.loads(body)
            except Exception as ex:
                log.error(f"JSON parse error for {date}: {ex}; body[:160]={body[:160]}")
                return None
    except Exception as ex:
        log.error(f"Request error for {date}: {ex}")
        return None

def _parse_utc(ts: str) -> dt.datetime:
    # e.g. "2025-09-23T00:00:00Z"
    return dt.datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc)

def _fmt_local(ts: dt.datetime) -> str:
    return ts.strftime("%Y-%m-%dT%H:%M:%S%z")

def _parse_updated_pair(val: str | None):
    """Return (utc_str, local_str) for updatedAt (or (None, None))."""
    if not val:
        return (None, None)
    try:
        ts = dt.datetime.fromisoformat(val.replace("Z", "+00:00"))
        return (ts.strftime("%Y-%m-%dT%H:%M:%S%z"), _fmt_local(ts.astimezone(EE_TZ)))
    except Exception as ex:
        log.warning(f"⚠️ Failed to parse updatedAt '{val}': {ex}")
        return (val, val)

def _status_for_area(data: dict | None, area: str) -> tuple[str, list]:
    """Extract status string for AREA from areaStates[]."""
    if not data:
        return ("Missing", [])
    area_states = data.get("areaStates") or []
    found = "Unknown"
    for row in area_states:
        st = row.get("state")
        areas = row.get("areas") or []
        if area in areas and st:
            found = st
            break
    return (found, area_states)

# ---------- price transforms ----------
def _is_offpeak(slot_start_local: dt.datetime) -> bool:
    # Weekend (Sat=5, Sun=6), night (<07 or >=22), or all-day holiday via HA calendar
    if slot_start_local.weekday() >= 5:
        return True
    h = slot_start_local.hour
    if h < 7 or h >= 22:
        return True
    if HOLIDAY_CALENDAR:
        cal_state = state.get(HOLIDAY_CALENDAR)
        attr = state.getattr(HOLIDAY_CALENDAR) or {}
        if cal_state == "on" and attr.get("all_day") and attr.get("start_time"):
            try:
                st = dt.datetime.fromisoformat(attr["start_time"].replace("Z", "+00:00")).astimezone(EE_TZ)
                if st.date() == slot_start_local.date():
                    return True
            except Exception:
                pass
    return False

def _apply_import(price: float, slot_start_local: dt.datetime) -> float:
    net = IMPORT["elektrilevi_88"] if _is_offpeak(slot_start_local) else IMPORT["elektrilevi_p2ev"]
    base = price + IMPORT["alexela_marginaal"] + IMPORT["taastuv"] + IMPORT["aktsiis"] + net
    return base * IMPORT["vat"]

def _apply_export(price: float) -> float:
    return price + EXPORT_MARGIN

def _map_import(raw_all):
    out = []
    for row in raw_all:
        try:
            start_dt = dt.datetime.strptime(row["start"], "%Y-%m-%dT%H:%M:%S%z")
            v = float(row["value"])
            out.append({**row, "value": _apply_import(v, start_dt)})
        except Exception:
            pass
    return out

def _map_export(raw_all):
    out = []
    for row in raw_all:
        try:
            v = float(row["value"])
            out.append({**row, "value": _apply_export(v)})
        except Exception:
            pass
    return out

# ---------- slot utilities ----------
def _expected_slots(start_local: dt.datetime, end_local: dt.datetime) -> int:
    # 15-minute buckets; handles DST (92/96/100) by computing exact 900s steps
    seconds = int((end_local - start_local).total_seconds())
    return max(0, seconds // (15 * 60))

def _count_slots_in_range(raw_all, start_local: dt.datetime, end_local: dt.datetime) -> int:
    n = 0
    for row in raw_all or []:
        try:
            sl = dt.datetime.strptime(row["start"], "%Y-%m-%dT%H:%M:%S%z")
            if start_local <= sl < end_local:
                n += 1
        except Exception:
            continue
    return n

def _slice_slots(raw_all, start_local: dt.datetime, end_local: dt.datetime):
    out = []
    for row in raw_all or []:
        try:
            sl = dt.datetime.strptime(row["start"], "%Y-%m-%dT%H:%M:%S%z")
            if start_local <= sl < end_local:
                out.append(row)
        except Exception:
            continue
    return out

# ---------- validation helpers (ADDED) ----------
def _validate_slice(raw_slice, start_local: dt.datetime, end_local: dt.datetime):
    """
    Validates that:
      - first slot starts at start_local
      - last slot ends at end_local
      - slots are 15-min contiguous (no gaps/overlaps)
      - count equals expected by exact seconds / 900
    Returns dict with booleans and diagnostics.
    """
    info = {
        "coverage_ok": False,
        "bounds_ok": False,
        "contiguous_ok": False,
        "count_ok": False,
        "first": None,
        "last": None,
        "count": 0,
        "expected": _expected_slots(start_local, end_local),
    }
    if not raw_slice:
        return info

    # parse times
    parsed = []
    for row in raw_slice:
        try:
            sl = dt.datetime.strptime(row["start"], "%Y-%m-%dT%H:%M:%S%z")
            el = dt.datetime.strptime(row["end"],   "%Y-%m-%dT%H:%M:%S%z")
            parsed.append((sl, el))
        except Exception:
            pass
    if not parsed:
        return info

    parsed.sort(key=lambda x: x[0])
    info["first"] = _fmt_local(parsed[0][0])
    info["last"]  = _fmt_local(parsed[-1][1])
    info["count"] = len(parsed)

    # bounds
    bounds_ok = (parsed[0][0] == start_local and parsed[-1][1] == end_local)
    info["bounds_ok"] = bounds_ok

    # contiguity
    contiguous_ok = True
    step = dt.timedelta(minutes=15)
    for i in range(1, len(parsed)):
        if parsed[i-1][1] != parsed[i][0]:
            contiguous_ok = False
            break
        if (parsed[i][0] - parsed[i-1][0]) != step:
            contiguous_ok = False
            break
    info["contiguous_ok"] = contiguous_ok

    # count & coverage
    info["count_ok"] = (len(parsed) == info["expected"])
    info["coverage_ok"] = (bounds_ok and contiguous_ok and info["count_ok"])
    return info

def _price_now_from_raw_all(raw_all, now_local):
    for row in raw_all or []:
        try:
            sl = dt.datetime.strptime(row["start"], "%Y-%m-%dT%H:%M:%S%z")
            el = dt.datetime.strptime(row["end"],   "%Y-%m-%dT%H:%M:%S%z")
            if sl <= now_local < el:
                return float(row["value"])
        except Exception:
            continue
    return None

def _combine_to_raw_all(combined, start_local, end_local):
    def fmt(ts): return ts.strftime("%Y-%m-%dT%H:%M:%S%z")
    out = [
        {"start": fmt(e["sl"]), "end": fmt(e["el"]), "value": e["p"]}
        for e in combined
        if (start_local <= e["sl"] < end_local)
    ]
    out.sort(key=lambda r: r["start"])
    # ensure contiguity by removing dupes/overlaps if any (defensive)
    clean = []
    last_end = None
    for r in out:
        if last_end and r["start"] != last_end:
            # gap/overlap → break; leave validator to decide fetch
            pass
        clean.append(r)
        last_end = r["end"]
    return clean

# ---------- main ----------
@time_trigger("startup")
@time_trigger("period(minute=15)")   # every 15 minutes, we decide inside whether to fetch
@service
async def nordpool_update(area=AREA, currency=CURRENCY, resolution=RESOLUTION):
    # overlap guard
    if getattr(nordpool_update, "_running", False):
        log.info("⏳ Previous nordpool_update still running; skipping this tick")
        return
    nordpool_update._running = True

    try:
        await asyncio.sleep(random.uniform(0.0, 3.0))  # jitter

        # Load previous cache/attrs
        prev_attrs = state.getattr(SENSOR_RAW) or {}
        cached_raw_all = prev_attrs.get("raw_all") or []
        prev_status_t  = prev_attrs.get("status_t")
        prev_status_tm = prev_attrs.get("status_tm")
        prev_today_date = prev_attrs.get("today_date_local")

        # Local day bounds
        now_utc   = dt.datetime.now(dt.timezone.utc)
        now_local = now_utc.astimezone(EE_TZ)
        today0    = now_local.replace(hour=0, minute=0, second=0, microsecond=0)
        tomorrow0 = today0 + dt.timedelta(days=1)
        day_after0= today0 + dt.timedelta(days=2)

        # Spill hour computation (difference EE vs CET at tomorrow boundary)
        spill_hours = int((EE_TZ.utcoffset(tomorrow0) - CET_TZ.utcoffset(tomorrow0)).total_seconds() // 3600)
        spill_hours = max(0, spill_hours)
        raw_all_end = day_after0 + dt.timedelta(hours=spill_hours)

        # Purge anything < today0 so yesterday can't linger
        if cached_raw_all:
            cached_raw_all = _slice_slots(cached_raw_all, today0, raw_all_end)

        # Cached slices & validation (ADDED)
        today_slice_cached    = _slice_slots(cached_raw_all, today0,   tomorrow0)
        tm_slice_cached       = _slice_slots(cached_raw_all, tomorrow0, day_after0)
        expected_today        = _expected_slots(today0, tomorrow0)
        expected_tomorrow     = _expected_slots(tomorrow0, day_after0)

        v_today = _validate_slice(today_slice_cached, today0, tomorrow0)
        v_tm    = _validate_slice(tm_slice_cached,    tomorrow0, day_after0)

        # Midnight rollover detection (ADDED)
        is_new_day = (prev_today_date != str(today0.date()))

        # Only chase tm after 13:00 local
        tomorrow_window = (now_local.hour >= 13)

        need_y_t = False
        need_tm  = False

        # Decide whether we need today/yesterday (ADDED: date validation)
        if is_new_day:
            need_y_t = True
        elif not v_today["coverage_ok"]:
            need_y_t = True
        elif (prev_status_t and prev_status_t != "Final"):
            need_y_t = True
        else:
            prev_status_t = prev_status_t or "Final"

        # Decide whether we need tomorrow (ADDED: date validation)
        tm_stale = not v_tm["coverage_ok"]
        if tomorrow_window:
            if tm_stale:
                need_tm = True
            elif (prev_status_tm and prev_status_tm != "Final"):
                need_tm = True

        combined = []
        data_y = data_t = data_tm = None

        # If nothing needed, we can compute current state from cache and exit early
        if not need_y_t and not need_tm and cached_raw_all:
            market_now = _price_now_from_raw_all(cached_raw_all, now_local)

            base_attrs = {
                **prev_attrs,
                "raw_all": cached_raw_all,
                "slots_today": v_today["count"],
                "slots_tomorrow": v_tm["count"],
                "slots_all": len(cached_raw_all),
                "expected_slots_today": expected_today,
                "expected_slots_tomorrow": expected_tomorrow,
                "today_first_slot": v_today["first"],
                "today_last_slot":  v_today["last"],
                "today_complete":   v_today["coverage_ok"],
                "tm_first_slot":    v_tm["first"],
                "tm_last_slot":     v_tm["last"],
                "tomorrow_complete": v_tm["coverage_ok"],
                "tm_stale": tm_stale,
                "today_date_local": str(today0.date()),
                "tomorrow_date_local_expected": str(tomorrow0.date()),
                "last_fetch_at_local": prev_attrs.get("last_fetch_at_local"),
            }

            # states
            state_import = _apply_import(market_now, now_local) if market_now is not None else None
            state_export = _apply_export(market_now)             if market_now is not None else None

            import_attrs = {
                "raw_all": _map_import(cached_raw_all),
                **{k: base_attrs.get(k) for k in (
                    "updatedAt_tm_utc","updatedAt_tm_local","status_y","status_t","status_tm",
                    "slots_today","slots_tomorrow","slots_all","expected_slots_today","expected_slots_tomorrow",
                    "today_first_slot","today_last_slot","today_complete",
                    "tm_first_slot","tm_last_slot","tomorrow_complete","tm_stale",
                    "today_date_local","tomorrow_date_local_expected","last_fetch_at_local",
                )},
                "unit_of_measurement": "EUR/kWh",
                "state_class": "measurement",
            }
            export_attrs = {
                "raw_all": _map_export(cached_raw_all),
                **{k: import_attrs[k] for k in import_attrs.keys() if k not in ("raw_all",)},
            }

            state.set(SENSOR_RAW,    market_now, new_attributes=base_attrs)
            state.set(SENSOR_IMPORT, state_import, new_attributes=import_attrs)
            state.set(SENSOR_EXPORT, state_export, new_attributes=export_attrs)
            log.info(
                "✅ No fetch needed; kept cache. Today=%d/%d (ok=%s), Tomorrow=%d/%d (ok=%s)",
                v_today["count"], expected_today, v_today["coverage_ok"],
                v_tm["count"], expected_tomorrow, v_tm["coverage_ok"]
            )
            return

        # We do need to fetch something
        # Figure CET delivery dates (y, t, tm)
        now_cet = now_utc.astimezone(CET_TZ)
        d_y  = (now_cet.date() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
        d_t  = (now_cet.date()).strftime("%Y-%m-%d")
        d_tm = (now_cet.date() + dt.timedelta(days=1)).strftime("%Y-%m-%d")

        try:
            async with aiohttp.ClientSession() as session:
                if need_y_t:
                    data_y = await _fetch_date(session, d_y,  area, currency, resolution)
                    data_t = await _fetch_date(session, d_t,  area, currency, resolution)
                if need_tm:
                    data_tm = await _fetch_date(session, d_tm, area, currency, resolution)
        except Exception as e:
            log.error(f"❌ Session error: {e} — keeping last values")
            return

        # Combine entries (only from datasets we fetched)
        for origin, data in (("y", data_y), ("t", data_t), ("tm", data_tm)):
            if not data:
                continue
            for e in data.get("multiIndexEntries", []):
                try:
                    su = _parse_utc(e["deliveryStart"])
                    eu = _parse_utc(e["deliveryEnd"])
                    p  = float(e["entryPerArea"][area]) / 1000.0  # EUR/MWh → EUR/kWh
                    sl = su.astimezone(EE_TZ)
                    el = eu.astimezone(EE_TZ)
                    combined.append({"su": su, "eu": eu, "sl": sl, "el": el, "p": p, "src": origin})
                except Exception as ex:
                    log.warning(f"⚠️ Skipping bad entry {e}: {ex}")

        # If we fetched nothing (e.g., only tm outside window), keep cache
        merged_raw_all = cached_raw_all[:]

        # If today/yesterday fetched → rebuild today's slice from API, keep any existing (validated) tomorrow slice from cache
        if need_y_t and combined:
            rebuilt_today = _combine_to_raw_all(combined, today0, tomorrow0)
            # keep only valid tomorrow slice from cache; else drop and wait for tm fetch window
            tomorrow_cached_slice = tm_slice_cached if v_tm["coverage_ok"] else []
            spill_tail = _slice_slots(cached_raw_all, day_after0, raw_all_end)
            merged_raw_all = rebuilt_today + tomorrow_cached_slice + spill_tail

        # If we fetched tm → rebuild tomorrow slice from API and merge into merged_raw_all
        if need_tm and combined:
            rebuilt_tm = _combine_to_raw_all(combined, tomorrow0, raw_all_end)
            kept_today = _slice_slots(merged_raw_all, today0, tomorrow0)
            merged_raw_all = kept_today + rebuilt_tm

        if not merged_raw_all:
            merged_raw_all = cached_raw_all[:]

        # Sort
        try:
            merged_raw_all.sort(key=lambda r: r["start"])
        except Exception:
            pass

        # Recompute counts + validity from merged
        today_slice  = _slice_slots(merged_raw_all, today0,   tomorrow0)
        tm_slice     = _slice_slots(merged_raw_all, tomorrow0, day_after0)
        v_today2     = _validate_slice(today_slice, today0, tomorrow0)
        v_tm2        = _validate_slice(tm_slice,    tomorrow0, day_after0)
        slots_today    = v_today2["count"]
        slots_tomorrow = v_tm2["count"]
        tm_stale2      = not v_tm2["coverage_ok"]

        # Determine statuses (keep previous if not fetched)
        status_y, areaStates_y   = _status_for_area(data_y,  area) if need_y_t else (prev_attrs.get("status_y") or "Unknown", prev_attrs.get("areaStates_y") or [])
        status_t, areaStates_t   = _status_for_area(data_t,  area) if need_y_t else (prev_attrs.get("status_t") or ("Final" if v_today2["coverage_ok"] else "Unknown"), prev_attrs.get("areaStates_t") or [])
        status_tm, areaStates_tm = _status_for_area(data_tm, area) if need_tm  else (prev_attrs.get("status_tm") or ("Final" if v_tm2["coverage_ok"] else "Missing"), prev_attrs.get("areaStates_tm") or [])

        # updatedAt for tomorrow only
        upd_tm_utc_prev   = prev_attrs.get("updatedAt_tm_utc")
        upd_tm_local_prev = prev_attrs.get("updatedAt_tm_local")
        upd_tm_utc, upd_tm_local = (upd_tm_utc_prev, upd_tm_local_prev)
        if data_tm:
            raw_upd = data_tm.get("updatedAt")
            if raw_upd:
                upd_tm_utc, upd_tm_local = _parse_updated_pair(raw_upd)

        # Current price
        market_now = _price_now_from_raw_all(merged_raw_all, now_local)
        if market_now is None and combined:
            for e in combined:
                if e["su"] <= now_utc < e["eu"]:
                    market_now = e["p"]
                    break

        # Build final attrs
        base_attrs = {
            "raw_all": merged_raw_all,
            "updatedAt_tm_utc": upd_tm_utc,
            "updatedAt_tm_local": upd_tm_local,
            "status_y": status_y,
            "status_t": status_t,
            "status_tm": status_tm,
            "areaStates_y": areaStates_y,
            "areaStates_t": areaStates_t,
            "areaStates_tm": areaStates_tm,
            "slots_today": slots_today,
            "slots_tomorrow": slots_tomorrow,
            "slots_all": len(merged_raw_all),
            "expected_slots_today": expected_today,
            "expected_slots_tomorrow": expected_tomorrow,
            "today_first_slot": v_today2["first"],
            "today_last_slot":  v_today2["last"],
            "today_complete":   v_today2["coverage_ok"],
            "tm_first_slot":    v_tm2["first"],
            "tm_last_slot":     v_tm2["last"],
            "tomorrow_complete": v_tm2["coverage_ok"],
            "tm_stale": tm_stale2,
            "today_date_local": str(today0.date()),
            "tomorrow_date_local_expected": str(tomorrow0.date()),
            "last_fetch_at_local": _fmt_local(now_local),
            "unit_of_measurement": "EUR/kWh",
            "state_class": "measurement",
        }

        # States & mapped arrays
        state_import = _apply_import(market_now, now_local) if market_now is not None else None
        state_export = _apply_export(market_now)            if market_now is not None else None

        import_attrs = {
            "raw_all": _map_import(merged_raw_all),
            **{k: base_attrs[k] for k in (
                "updatedAt_tm_utc","updatedAt_tm_local","status_y","status_t","status_tm",
                "slots_today","slots_tomorrow","slots_all","expected_slots_today","expected_slots_tomorrow",
                "today_first_slot","today_last_slot","today_complete",
                "tm_first_slot","tm_last_slot","tomorrow_complete","tm_stale",
                "today_date_local","tomorrow_date_local_expected","last_fetch_at_local",
            )},
            "unit_of_measurement": "EUR/kWh",
            "state_class": "measurement",
        }
        export_attrs = {
            "raw_all": _map_export(merged_raw_all),
            **{k: import_attrs[k] for k in import_attrs.keys() if k not in ("raw_all",)},
        }

        # Push states
        state.set(SENSOR_RAW,    market_now, new_attributes=base_attrs)
        state.set(SENSOR_IMPORT, state_import, new_attributes=import_attrs)
        state.set(SENSOR_EXPORT, state_export, new_attributes=export_attrs)

        log.info(
            "✅ Fetched as needed: today %d/%d (ok=%s), tomorrow %d/%d (ok=%s); status t=%s, tm=%s",
            slots_today, expected_today, v_today2["coverage_ok"],
            slots_tomorrow, expected_tomorrow, v_tm2["coverage_ok"],
            status_t, status_tm
        )

    finally:
        nordpool_update._running = False
