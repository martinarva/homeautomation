"""
NordPool 15-min (EE) â€” base, import & export prices via Pyscript

What you get (entities created beforehand in HA UI as empty helpers):
- sensor.nordpool_15_min_raw_pyscript
  - state: current 15-min **market** price (â‚¬/kWh)
  - attributes:
      raw_today     : 15-min slots for local "today" (Europe/Tallinn)
      raw_tomorrow  : 15-min slots for local "tomorrow" (only when published)
      raw_all       : today + tomorrow (includes the extra spill hour across CETâ†”EE)
      updatedAt     : timestamp in Europe/Tallinn of the **tomorrow** dataset publish time
      unit_of_measurement, state_class

- sensor.nordpool_15_min_import_pyscript
  - state: current 15-min **import** price after tariffs (â‚¬/kWh)
  - attributes: same 15-min arrays, tariffed

- sensor.nordpool_15_min_export_pyscript
  - state: current 15-min **export** price (market price + export margin; no VAT), â‚¬/kWh
  - attributes: same 15-min arrays, export-adjusted

Scheduling:
- Runs on startup
- Every hour outside 14:00â€“15:00 local
- Every 15 minutes during 14:00â€“15:00 local 
  (prices are typically published in this window; we poll more often only to catch the update promptly)
  
Manual run:
- Developer Tools â†’ Services â†’ call service: nordpool_update

Install:
- Save as apps/pyscript/nordpool_15min.py (or any .py under pyscript/)
- Create the 3 helper sensors listed above (IDs must match)
- (Optional) Set HOLIDAY_CALENDAR below to your HA holiday calendar entity_id
"""

import json
import datetime as dt
import aiohttp
import asyncio
import random

# ---------- Configuration ----------
HOLIDAY_CALENDAR = "calendar.estonia"  # set to None to disable holiday logic (weekend/night remains)
AREA      = "EE"
CURRENCY  = "EUR"
RESOLUTION= 15  # minutes; 15 from 2025-10-01

# Import tariff components (all values in EUR/kWh)
IMPORT = {
    "alexela_marginaal": 0.0000,
    "taastuv":          0.0084,
    "aktsiis":          0.0021,
    "elektrilevi_p2ev": 0.0369,  # day tariff
    "elektrilevi_88":   0.0210,  # night/weekend/holiday
    "vat":              1.24,    # multiplier
}

# Export margin (negative means we subtract margin from market price); no VAT on export
EXPORT_MARGIN = -0.00167  # EUR/kWh

# Target helper entities (must exist)
SENSOR_RAW    = "sensor.nordpool_15_min_raw_pyscript"
SENSOR_IMPORT = "sensor.nordpool_15_min_import_pyscript"
SENSOR_EXPORT = "sensor.nordpool_15_min_export_pyscript"

# ---------- Timezones ----------
try:
    from zoneinfo import ZoneInfo
    EE_TZ  = ZoneInfo("Europe/Tallinn")
    CET_TZ = ZoneInfo("Europe/Paris")   # CET/CEST (market calendar)
except Exception:
    EE_TZ  = dt.timezone(dt.timedelta(hours=3))
    CET_TZ = dt.timezone(dt.timedelta(hours=1))

API_BASE = "https://dataportal-api.nordpoolgroup.com/api/DayAheadPriceIndices"
HEADERS  = {
    "Accept": "application/json",
    "User-Agent": "HomeAssistant-Pyscript-Nordpool/1.1 (+nordpool_15_min_raw)"
}

# ---------- Helpers ----------
async def _fetch_date(session, date, area, currency, resolution) -> dict | None:
    url = (
        f"{API_BASE}?date={date}"
        f"&market=DayAhead&indexNames={area}&currency={currency}&resolutionInMinutes={resolution}"
    )
    log.info(f"ðŸ”— GET {url}")
    try:
        async with session.get(url, headers=HEADERS, timeout=30) as resp:
            body = await resp.text()
            if resp.status != 200:
                log.warning(f"HTTP {resp.status} for {date}; body[:200]={body[:200]}")
                return None
            try:
                return json.loads(body)
            except Exception as ex:
                log.error(f"JSON parse error for {date}: {ex}; body[:160]={body[:160]}")
                return None
    except Exception as ex:
        log.error(f"Request error for {date}: {ex}")
        return None

def _parse_utc(ts: str) -> dt.datetime:
    # e.g. "2025-09-23T00:00:00Z"
    return dt.datetime.strptime(ts, "%Y-%m-%dT%H:%M:%SZ").replace(tzinfo=dt.timezone.utc)

def _fmt_local(ts: dt.datetime) -> str:
    return ts.strftime("%Y-%m-%dT%H:%M:%S%z")

def _parse_updated_local(val: str | None) -> str | None:
    if not val:
        return None
    try:
        ts = dt.datetime.fromisoformat(val.replace("Z", "+00:00"))
        return _fmt_local(ts.astimezone(EE_TZ))
    except Exception as ex:
        log.warning(f"âš ï¸ Failed to parse updatedAt '{val}': {ex}")
        return val

def _is_offpeak(slot_start_local: dt.datetime) -> bool:
    # Weekend (Sat=5, Sun=6), night (<07 or >=22), or all-day holiday via HA calendar entity
    if slot_start_local.weekday() >= 5:
        return True
    h = slot_start_local.hour
    if h < 7 or h >= 22:
        return True
    if HOLIDAY_CALENDAR:
        cal_state = state.get(HOLIDAY_CALENDAR)
        cal_attr  = state.getattr(HOLIDAY_CALENDAR) or {}
        # Consider "all_day" and same local date of the start_time
        st = cal_attr.get("start_time")
        all_day = cal_attr.get("all_day")
        if cal_state == "on" and all_day and st:
            try:
                st_dt = dt.datetime.fromisoformat(st.replace("Z", "+00:00")).astimezone(EE_TZ)
                if st_dt.date() == slot_start_local.date():
                    return True
            except Exception:
                pass
    return False

def _apply_import(price: float, slot_start_local: dt.datetime) -> float:
    # price = market â‚¬/kWh â†’ add tariffs, then VAT
    if _is_offpeak(slot_start_local):
        network = IMPORT["elektrilevi_88"]
    else:
        network = IMPORT["elektrilevi_p2ev"]
    base = price + IMPORT["alexela_marginaal"] + IMPORT["taastuv"] + IMPORT["aktsiis"] + network
    return base * IMPORT["vat"]

def _apply_export(price: float) -> float:
    # price = market â‚¬/kWh â†’ add (negative) margin; no VAT
    return price + EXPORT_MARGIN

def _build_attr_from(combined, day_start_local, day_end_local, allowed_sources):
    # Build 15-min array between [day_start_local, day_end_local) using entries whose source in allowed_sources
    res = []
    for e in combined:
        if e["src"] not in allowed_sources:
            continue
        if day_start_local <= e["sl"] < day_end_local:
            res.append({"start": _fmt_local(e["sl"]), "end": _fmt_local(e["el"]), "value": e["p"]})
    return res

def _map_import(array):
    out = []
    for row in array:
        try:
            start_dt = dt.datetime.strptime(row["start"], "%Y-%m-%dT%H:%M:%S%z")
            val = float(row["value"])
            out.append({**row, "value": _apply_import(val, start_dt)})
        except Exception:
            pass
    return out

def _map_export(array):
    out = []
    for row in array:
        try:
            val = float(row["value"])
            out.append({**row, "value": _apply_export(val)})
        except Exception:
            pass
    return out


@time_trigger("startup")
@time_trigger("cron(0 0-13,15-23 * * *)")   # top of the hour, excluding 14:00
@time_trigger("cron(0,15,30,45 14 * * *)")  # every 15 min during 14:00â€“14:59
@service
async def nordpool_update(area=AREA, currency=CURRENCY, resolution=RESOLUTION):
    # overlap guard
    if getattr(nordpool_update, "_running", False):
        log.info("â³ Previous nordpool_update still running; skipping this tick")
        return
    nordpool_update._running = True

    try:
        await asyncio.sleep(random.uniform(0.0, 3.0))  # jitter

        # CET delivery dates
        now_cet = dt.datetime.now(dt.timezone.utc).astimezone(CET_TZ)
        d_y  = (now_cet.date() - dt.timedelta(days=1)).strftime("%Y-%m-%d")
        d_t  = (now_cet.date()).strftime("%Y-%m-%d")
        d_tm = (now_cet.date() + dt.timedelta(days=1)).strftime("%Y-%m-%d")

        # fetch
        try:
            async with aiohttp.ClientSession() as session:
                data_y  = await _fetch_date(session, d_y,  area, currency, resolution)
                data_t  = await _fetch_date(session, d_t,  area, currency, resolution)
                data_tm = await _fetch_date(session, d_tm, area, currency, resolution)
        except Exception as e:
            log.error(f"âŒ Session error: {e} â€” keeping last values")
            return

        # combine entries
        combined = []
        for origin, data in (("y", data_y), ("t", data_t), ("tm", data_tm)):
            if not data:
                continue
            for e in data.get("multiIndexEntries", []):
                try:
                    su = _parse_utc(e["deliveryStart"])
                    eu = _parse_utc(e["deliveryEnd"])
                    p  = float(e["entryPerArea"][area]) / 1000.0
                    sl = su.astimezone(EE_TZ)
                    el = eu.astimezone(EE_TZ)
                    combined.append({"su": su, "eu": eu, "sl": sl, "el": el, "p": p, "src": origin})
                except Exception as ex:
                    log.warning(f"âš ï¸ Skipping bad entry {e}: {ex}")

        if not combined:
            log.warning("âš ï¸ No entries from any day â€” keeping last values")
            return

        # current state
        now_utc  = dt.datetime.now(dt.timezone.utc)
        now_loc  = now_utc.astimezone(EE_TZ)
        today0   = now_loc.replace(hour=0, minute=0, second=0, microsecond=0)
        tomorrow0= today0 + dt.timedelta(days=1)
        day_after0 = today0 + dt.timedelta(days=2)

        state_market = None
        for e in combined:
            if e["su"] <= now_utc < e["eu"]:
                state_market = e["p"]
                break

        # arrays (base)
        def fmt(ts): return ts.strftime("%Y-%m-%dT%H:%M:%S%z")
        raw_today = [
            {"start": fmt(e["sl"]), "end": fmt(e["el"]), "value": e["p"]}
            for e in combined if (today0 <= e["sl"] < tomorrow0) and (e["src"] in ("y","t"))
        ]
        raw_tomorrow_tm = [
            {"start": fmt(e["sl"]), "end": fmt(e["el"]), "value": e["p"]}
            for e in combined if (tomorrow0 <= e["sl"] < day_after0) and (e["src"] == "tm")
        ]
        raw_all = [
            {"start": fmt(e["sl"]), "end": fmt(e["el"]), "value": e["p"]}
            for e in combined if (today0 <= e["sl"] < day_after0)
        ]

        # updatedAt = only from tomorrow payload, in local time; else keep previous
        prev_attrs = state.getattr(SENSOR_RAW) or {}
        prev_updated_local = prev_attrs.get("updatedAt")
        tm_updated_local = _parse_updated_local((data_tm or {}).get("updatedAt"))

        base_attrs = {
            "raw_today": raw_today,
            "raw_tomorrow": raw_tomorrow_tm,
            "raw_all": raw_all,
            "updatedAt": tm_updated_local or prev_updated_local,
            "unit_of_measurement": "EUR/kWh",
            "state_class": "measurement",
        }

        # import/export states
        state_import = _apply_import(state_market, now_loc) if state_market is not None else None
        state_export = _apply_export(state_market)          if state_market is not None else None

        # import/export arrays (no lambdas â†’ no coroutines)
        import_today    = _map_import(raw_today)
        import_tomorrow = _map_import(raw_tomorrow_tm)
        import_all      = _map_import(raw_all)

        export_today    = _map_export(raw_today)
        export_tomorrow = _map_export(raw_tomorrow_tm)
        export_all      = _map_export(raw_all)

        import_attrs = {
            "raw_today": import_today,
            "raw_tomorrow": import_tomorrow,
            "raw_all": import_all,
            "updatedAt": base_attrs["updatedAt"],
            "unit_of_measurement": "EUR/kWh",
            "state_class": "measurement",
        }
        export_attrs = {
            "raw_today": export_today,
            "raw_tomorrow": export_tomorrow,
            "raw_all": export_all,
            "updatedAt": base_attrs["updatedAt"],
            "unit_of_measurement": "EUR/kWh",
            "state_class": "measurement",
        }

        state.set(SENSOR_RAW,    state_market, new_attributes=base_attrs)
        state.set(SENSOR_IMPORT, state_import, new_attributes=import_attrs)
        state.set(SENSOR_EXPORT, state_export, new_attributes=export_attrs)

        log.info(
            "âœ… Updated: base=%s â‚¬/kWh, import=%s â‚¬/kWh, export=%s â‚¬/kWh; today=%d, tomorrow=%d, all=%d",
            state_market, state_import, state_export,
            len(raw_today), len(raw_tomorrow_tm), len(raw_all)
        )

    finally:
        nordpool_update._running = False
